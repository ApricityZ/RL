---
date: 2023-11-15 09:55:53
status: doing
excerpt: 开源社区强化学习随笔记录
tags:
  - RL
  - JoyRL
aliases: 
rating: ⭐⭐
---
蘑菇书侧重于理论详细讲解，$JoyRL$更加的简练，重视代码实践。

试错学习是强化学习的最鲜明要素，其包括：尝试、错误、结果、学习。提及试错学习，不免想到强化学习的一个主题：安全强化学习。既要试错，但又不能错的太过分。

每次尝试都是一次**决策**（decision），但是决策带来的结果可好可坏，可短期体现，也可长期才体现。如何衡量结果的好坏？答案是**奖励**（reward），奖励的相对值越小，说明结果越坏，称为惩罚（punishment）。由于结果可能是短期也可能是长期，所以要考虑累计奖励，强化学习的目标是通过决策来实现累计奖励的最大化，这个过程称为**序列决策**（sequential decision making）。

由于强化学习是试错过程，现实应用中的实验成本较大，一是设备较贵、二是容易引起设备损坏。为此，建立仿真环境来节约成本。但是，这种方法的弊端就是仿真和现实之间存在较大差距，使得强化学习算法较难落地。

RL在游戏、机器人、金融领域均有应用。

强化学习方向：1. 多智能体强化学习，这个方向也是我今后学习的方向；2. 从数据中学习；3. 探索策略、实时环境、多任务强化学习。

学习强化学习的预备知识：一定的机器学习基础，如基本的线性代数、概率论、书路统计等；传统强化学习部分不涉及神经网络，虽然不常用但是其中的思想和技巧很重要。深度学习在强化学习的环境下的作用是提供强大的函数拟合能力。最后，强化学习算法虽多，但基本上基于价值的和基于策略梯度的算法两类即可将之涵盖。

### Chapter 2 Markov Decision Process

序列决策问题的本质是与环境交互的过程中学习到一个目标的过程，马尔可夫决策过程能够以数学的形式来表达序列决策过程。（预备知识：概率论，重点是条件概率、全概率期望公式）

![[Pasted image 20231115104641.png|500]]
										 马尔可夫决策过程中智能体与环境的交互过程

智能体负责决策、执行并且学习，环境是指于智能体进行交互的一切外在事物，不包括智能体本身。

智能体和环境之间以一系列**离散的时步**（time step）进行交互，用$t$ 来表示。其过程可以描述为，在 $t$ 时步，智能体根据当前环境的状态 $s_t$ ,执行动作 $a_t$ ,执行完动作后会收到环境返回的奖励 $r_{t+1}$，由于环境和智能体是一个交互的过程，所以环境在收到动作 $a_t$ 后会发生变化，随之智能体在时步$t+1$ 时观察到的环境的状态变为 $s_{t+1}$  ，如此循环下去，可以得到一串轨迹：

$$ s_0, a_0,r_1,s_1,a_1,r_2,...,s_t,s_t,r_{t+1},...$$ 
需要注意的是：1、离散时步可以拓展为连续时步；2、t与现实实践无关，相邻t间隔取决于智能体每次交互 并获得反馈所需要的时间。例如考试和取得成绩的时间间隔，此时就很漫长。（很贴切，现在我就在等待成绩。）3、关于奖励是$r_{t+1}$ 而不是$r_t$ 的理解：在状态$s_t$ 执行完动作 $a_t$ 后才可以取得奖励，比如说在 $t=0$ 时步考试，但是奖励--成绩是在时步 $t=1$ 时收到的。

#### 马尔可夫性质

数学形式：
$$P(s_{t+1}|s_t) = P(s_{t+1}|s_0,s_1,...,s_t)$$
含义：下一个状态仅与当前状态有关，而与历史状态无关。即，当前状态 $s_t$ 提供的信息足够判断下一时刻的状态 $s_{t+1}$ 。

需要注意的是，实际问题中很多例子不满足马尔可夫性质，即：不仅取决于当前状态，还依赖于历史状态。但这并不意味着强化学习不能用来解决此类问题，当解决问题不能严格满足马尔可夫性质的条件时，是可以结合其他的方法来辅助强化学习进行决策的。

#### 回报

将累积的奖励称为**回报**（Return），注意与奖励（Reward）进行区分。

- 有限步情况下的回报：
$$
G_t = r_{t+1} + r_{t+2} + ... +r_T
$$
其中，T表示最后一个时步，即每回合（episode）的最大步数。但是在实际问题中，任务可能是持续性任务，此时 $T=\infty$ , 此时有公式改变。

- 无限步情况下的回报
$$ 
\tag{2.4}
G_t = r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + \cdots = \sum_{k=0}^\infty \gamma^k r_{t+k+1}
$$
其中，$\gamma$ 取值在0到1之间，当其为0时，代表只关心当前的奖励，当其为1时，将未来的长期奖励与当前奖励视为同等重要，故其代表了未来奖励的重要程度。

对 $\text(2.4)$ 变形，可得：
$$
\tag{2.5}
\begin{aligned}
G_t & \doteq r_{t+1}+\gamma r_{t+2}+\gamma^2 r_{t+3}+\gamma^3 r_{t+4}+\cdots \\
& =r_{t+1}+\gamma\left(r_{t+2}+\gamma r_{t+3}+\gamma^2 r_{t+4}+\cdots\right) \\
& =r_{t+1}+\gamma G_{t+1}
\end{aligned}
$$
这个式子将当前时步回报$G_t$ 和下一时步的回报 $G_{t+1}$ 联系了起来。

#### 状态转移矩阵
此处考虑状态有限的情况，当状态无限多时，考虑使用泊松（Poisson）过程进行建模。

>**有时智能体和环境的角色是能相互对调的，只要能各自建模成马尔可夫决策过程即可**

我们可以用一种状态流向图来表示智能体与环境交互过程中的走向。举个例子，假设学生正在上课，一般来讲从老师的角度来说学生会有三种状态，认真听讲、玩手机和睡觉，分别用$s_1$ ，$s_2$ 和$s_3$ 表示,下图为状态转移图：
![[Pasted image 20231115114154.png|400]]

由于状态有限，且满足马尔可夫性质，故可以以表格形式表示状态间的转移概率：
<div style="text-align: center;">
  <div style="display: table; margin: 0 auto;">
    <table>
      <tr>
        <th> $\space$ </th>
        <th>$S_{t+1} = s_1$</th>
        <th>$S_{t+1} = s_2$</th>
        <th>$S_{t+1} = s_3$</th>
      </tr>
      <tr>
        <td>$S_t = s_1$</td>
        <td>$0.2$</td>
        <td>$0.4$</td>
        <td>$0.4$</td>
      </tr>
      <tr>
        <td>$S_t = s_2$</td>
        <td>$0.2$</td>
        <td>$0.5$</td>
        <td>$0.3$</td>
      </tr>
      <tr>
        <td>$S_t = s_3$</td>
        <td>$0.1$</td>
        <td>$0.3$</td>
        <td>$0.6$</td>
      </tr>
    </table>
  </div>
  <div>表 $2.1$ ：马尔可夫状态表</div>
</div>

进一步可以用矩阵形式表达：

$$
\begin{gather*}
  P_{ss'}=
     \begin{bmatrix}
         0.2 & 0.4 & 0.4\\
         0.2 & 0.5 & 0.3\\
         0.1 & 0.3 & 0.6\\
     \end{bmatrix}
\end{gather*}
$$

此矩阵即为**状态转移矩阵**（State Transition Matrix），更加一般化，得：

$$

P_{ss'}=\begin{pmatrix}
p_{11} & p_{12} & \cdots & p_{1n}\\
p_{21} & p_{22} & \cdots & p_{2n}\\
\vdots & \vdots & \ddots & \vdots \\
p_{n1} & p_{n2} & \cdots & p_{nn}
\end{pmatrix}
$$

需要注意的是，**状态转移矩阵是环境的一部分，与智能体无关**，这也是接下来得有模型和无模型中用到的概念。上述过程由于不存在动作和奖励，故被称为**马尔科夫链**，加入奖励后，形成**马尔科夫奖励过程**，在此基础上添加动作元素，即形成**马尔可夫决策过程**。MDP的常用写法是用一个五元组来表示，$<S,A,R,P,\gamma>$，其中，S表示状态集合（状态空间），A表示动作空间，R表示奖励函数，P表示状态转移矩阵，$\gamma$ 表示折扣因子。

**练习题**
1. 强化学习所解决的问题一定要严格满足马尔可夫性质吗？请举例说明。
    不一定严格满足。如围棋问题中，不仅要考虑当前棋局，还要考虑历史因素。
2. 马尔可夫决策过程主要包含哪些要素？
    马尔可夫决策 $<S,A,R,P,\gamma>$ 主要包含状态空间 $S$、动作空间 $A$、奖励函数 $R$、状态转移矩阵 $P$、折扣因子 $\gamma$ 等要素，其中状态转移矩阵 $P$ 是环境的一部分，而其他要素是智能体的一部分。在实际应用中，通常还考虑值函数 $V$ 和策略函数 $\pi$ 等要素，值函数用于某个状态下的长期累积奖励，策略函数用于某个状态下的动作选择。
3. 马尔可夫决策过程与金融科学中的马尔可夫链有什么区别与联系？
	马尔可夫链是一个随机过程，其下一个状态只依赖于当前状态而不受历史状态的影响，即满足马尔可夫性质。马尔可夫链由状态空间、初始状态分布和状态转移概率矩阵组成。马尔可夫决策过程是一种基于马尔可夫链的决策模型，它包含了状态、行动、转移概率、奖励、值函数和策略等要素。马尔可夫决策过程中的状态和状态转移概率满足马尔可夫性质，但区别在于它还包括了行动、奖励、值函数和策略等要素，用于描述在给定状态下代理如何选择行动以获得最大的长期奖励。

### Chap03 动态规划

>**动态规划**（英语：Dynamic programming，简称DP）是一种在[数学](https://zh.wikipedia.org/wiki/%E6%95%B0%E5%AD%A6 "数学")、[管理科学](https://zh.wikipedia.org/wiki/%E7%AE%A1%E7%90%86%E7%A7%91%E5%AD%A6 "管理科学")、[计算机科学](https://zh.wikipedia.org/wiki/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6 "计算机科学")、[经济学](https://zh.wikipedia.org/wiki/%E7%BB%8F%E6%B5%8E%E5%AD%A6 "经济学")和[生物信息学](https://zh.wikipedia.org/wiki/%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6 "生物信息学")中使用的，通过把原问题分解为相对简单的子问题的方式求解复杂问题的方法。
动态规划常常适用于有重叠子问题[1](https://zh.wikipedia.org/zh-hans/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92#cite_note-1)和[最优子结构](https://zh.wikipedia.org/w/index.php?title=%E6%9C%80%E4%BC%98%E5%AD%90%E7%BB%93%E6%9E%84&action=edit&redlink=1)性质的问题，动态规划方法所耗时间往往远少于朴素解法。

#### 价值函数

状态价值函数（state-value function）

$$
v_\pi(s) \doteq \mathbb E_\pi[G_t|S_t = s]
$$
将 $G_t$ 代入，有：
$$
v_\pi(s) \doteq \mathbb E[R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\cdots]
$$
在状态 $S_t=s$ 时，执行动作 $a$ ，会得到回报的均值，称之为动作价值函数（action-value function）
$$
Q_\pi (s,a) \doteq \mathbb E_\pi [G_t|S_t=s,A_t =a ]
$$
易得：
$$
v_\pi (s) = \sum_{a \in A} \pi(a \mid s) Q_\pi(s,a)
$$
上式可由全概率公式来得到。

#### 贝尔曼方程

由状态价值函数的数学形式可得：
$$
\begin{aligned}
v_\pi(s) & = \mathbb{E}_\pi[G_t \mid S_t = s] \\
& = \mathbb E_\pi [R_{t+1}+\gamma R_{t+2}+{\gamma}^2 R_{t+3}+ \cdots \mid S_t=s] \\
& =\mathbb{E}\left[R_{t+1} \mid s_t=s\right]+\gamma \mathbb{E}\left[R_{t+2}+\gamma R_{t+3}+\gamma^2 R_{t+4}+\cdots \mid S_t=s\right] \\

& =R(s)+\gamma \mathbb{E}\left[G_{t+1} \mid S_t=s\right] \\
& =R(s)+\gamma \mathbb{E}\left[V_{\pi}\left(s_{t+1}\right) \mid S_t=s\right] \\
& =R(s)+\gamma \sum_{s^{\prime} \in S} P\left(S_{t+1}=s^{\prime} \mid S_{t}=s\right) V_{\pi}\left(s^{\prime}\right)\\
& =R(s)+\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s\right) V_{\pi}\left(s^{\prime}\right)
\end{aligned}
$$

上式即为贝尔曼公式，贝尔曼公式将前后两个状态联系起来，便于递归地解决问题。

类似，动作价值函数贝尔曼方程为：
$$
Q_{\pi}(s,a) = R(s,a) + \gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s,a\right) \sum_{a^{\prime} \in A} \pi\left(a^{\prime} \mid s ^{\prime} \right)Q_{\pi}\left(s^{\prime},a'\right)
$$
价值函数是按照某种策略 $\pi$ 进行决策得到的累计回报期望。在最优策略下状态价值函数和动作价值函数都是最优的。为了使累计回报最大，则有：
$$
\begin{aligned}
V^{*}(s)&=\max _a \mathbb{E}\left[R_{t+1}+\gamma V^{*}\left(S_{t+1}\right) \mid S_t=s, A_t=a\right] \\
&=\max_a \sum_{s',r}p(s',r|s,a)[r+\gamma V^{*}(s')] 
\end{aligned}
$$
类似的，对于动作价值函数有：
$$
\begin{aligned}
Q^{*}(s, a) & =\mathbb{E}\left[R_{t+1}+\gamma \max _{a^{\prime}} Q^{*}\left(S_{t+1}, a^{\prime}\right) \mid S_t=s, A_t=a\right] \\
& =\sum_{s^{\prime}, r} p\left(s^{\prime}, r \mid s, a\right)\left[r+\gamma \max _{a^{\prime}} Q^{*}\left(s^{\prime}, a^{\prime}\right)\right]
\end{aligned}
$$

下面，根据上述贝尔曼最优方程，有两个主要的算法：

价值迭代算法和策略迭代算法。
